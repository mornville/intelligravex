{% extends "base.html" %}
{% block content %}
  <div class="row">
    <h1 style="margin: 0;">{{ bot.name }}</h1>
    <span class="pill">UUID: <code>{{ bot.id }}</code></span>
    <div class="right"></div>
    <a class="pill" href="/bots">Back</a>
  </div>

  <div class="grid" style="margin-top: 14px;">
    <form class="card" method="post" action="/bots/{{ bot.id }}">
      <h2>Configuration</h2>

      <label>Name</label>
      <input type="text" name="name" required value="{{ bot.name }}" />

      <label>OpenAI model</label>
      <select name="openai_model">
        {% for m in options.openai_models %}
          <option value="{{ m }}" {% if m == bot.openai_model %}selected{% endif %}>{{ m }}</option>
        {% endfor %}
      </select>

      <label>OpenAI key</label>
      <select name="openai_key_id">
        <option value="" {% if not bot.openai_key_id %}selected{% endif %}>(use env OPENAI_API_KEY)</option>
        {% for k in keys %}
          <option value="{{ k.id }}" {% if bot.openai_key_id == k.id %}selected{% endif %}>{{ k.name }} — {{ k.hint }}</option>
        {% endfor %}
      </select>

      <label>System prompt</label>
      <textarea name="system_prompt" required>{{ bot.system_prompt }}</textarea>

      <div class="row">
        <div style="flex: 1;">
          <label>Conversation start message</label>
          <select name="start_message_mode" id="start_message_mode">
            <option value="llm" {% if bot.start_message_mode == "llm" %}selected{% endif %}>LLM generated</option>
            <option value="static" {% if bot.start_message_mode == "static" %}selected{% endif %}>Static</option>
          </select>
        </div>
      </div>

      <label>Start message text (optional)</label>
      <textarea name="start_message_text" id="start_message_text" placeholder="will be generated by LLM">{{ bot.start_message_text or "" }}</textarea>

      <div class="row">
        <div style="flex: 1;">
          <label>ASR language</label>
          <select name="language">
            {% for l in options.languages %}
              <option value="{{ l }}" {% if l == bot.language %}selected{% endif %}>{{ l }}</option>
            {% endfor %}
          </select>
        </div>
        <div style="flex: 1;">
          <label>TTS language</label>
          <select name="tts_language">
            {% for l in options.languages %}
              <option value="{{ l }}" {% if l == bot.tts_language %}selected{% endif %}>{{ l }}</option>
            {% endfor %}
          </select>
        </div>
      </div>

      <div class="row">
        <div style="flex: 1;">
          <label>Whisper model</label>
          <select name="whisper_model">
            {% for m in options.whisper_models %}
              <option value="{{ m }}" {% if m == bot.whisper_model %}selected{% endif %}>{{ m }}</option>
            {% endfor %}
          </select>
        </div>
        <div style="flex: 1;">
          <label>Whisper device</label>
          <select name="whisper_device">
            {% for d in options.whisper_devices %}
              <option value="{{ d }}" {% if d == bot.whisper_device %}selected{% endif %}>{{ d }}</option>
            {% endfor %}
          </select>
        </div>
      </div>

      <label>XTTS v2 model</label>
      <select id="xtts_model" name="xtts_model">
        {% for m in options.xtts_models %}
          <option value="{{ m }}" {% if m == bot.xtts_model %}selected{% endif %}>{{ m }}</option>
        {% endfor %}
      </select>

      <div class="row">
        <div style="flex: 1;">
          <label>Speaker ID (optional)</label>
          <select id="speaker_id" name="speaker_id">
            <option value="">(auto)</option>
          </select>
        </div>
        <div style="flex: 1;">
          <label>Speaker WAV path (optional)</label>
          <input type="text" name="speaker_wav" value="{{ bot.speaker_wav or '' }}" />
        </div>
      </div>

      <div class="row">
        <div style="flex: 1;">
          <label>TTS chunk min chars</label>
          <input type="number" name="tts_chunk_min_chars" value="{{ bot.tts_chunk_min_chars }}" min="1" />
        </div>
        <div style="flex: 1;">
          <label>TTS chunk max chars</label>
          <input type="number" name="tts_chunk_max_chars" value="{{ bot.tts_chunk_max_chars }}" min="10" />
        </div>
      </div>

      <label><input type="checkbox" name="tts_split_sentences" {% if bot.tts_split_sentences %}checked{% endif %} /> Let TTS split sentences (slower first-audio)</label>

      <div class="row" style="margin-top: 12px;">
        <button type="submit">Save</button>
        <button class="danger" formaction="/bots/{{ bot.id }}/delete" formmethod="post">Delete</button>
      </div>
    </form>

    <div class="card">
      <div class="row">
        <h2 style="margin:0;">Test Conversation (Mic)</h2>
        <div class="right"></div>
        <a class="pill" href="/bots/{{ bot.id }}/conversations">Conversations</a>
      </div>
      <div class="muted">Press “Start”, speak, then “Stop”. This stores a test conversation for this bot.</div>

      <div class="row" style="margin-top: 10px;">
        <label style="margin: 0;"><input type="checkbox" id="speak" checked /> Speak</label>
        <span class="pill" id="convPill">conversation: (new)</span>
        <span class="pill" id="statusPill">status: idle</span>
        <span class="pill" id="latencyPill">latency: —</span>
        <div class="right"></div>
        <button type="button" id="newConvBtn">New conversation</button>
        <button type="button" id="toggleBtn" class="recBtn">Start</button>
      </div>

      <div id="chat" style="margin-top: 12px; display:flex; flex-direction:column; gap:10px;"></div>

      <script>
        const botId = "{{ bot.id }}";
        const chat = document.getElementById("chat");
        const speak = document.getElementById("speak");
        const toggleBtn = document.getElementById("toggleBtn");
        const newConvBtn = document.getElementById("newConvBtn");
        const convPill = document.getElementById("convPill");
        const statusPill = document.getElementById("statusPill");
        const latencyPill = document.getElementById("latencyPill");

        let conversationId = "";
        let mediaStream = null;
        let audioCtx = null;
        let processor = null;
        let sourceNode = null;
        let recorderIsWorklet = false;
        let chunks = [];
        let inputSampleRate = 48000;
        let isRecording = false;
        let isBusy = false;

	        // Playback scheduling
	        let playCtx = null;
	        let nextPlayTime = 0;
        let ws = null;
        let wsHandlers = new Map(); // reqId -> handler(msg)

	        function setConversation(id) {
	          conversationId = id || "";
	          convPill.textContent = conversationId ? `conversation: ${conversationId}` : "conversation: (new)";
	        }

	        function setStatus(stage) {
	          statusPill.textContent = `status: ${stage}`;
            statusPill.style.borderColor = stage === "recording" ? "rgba(255, 94, 140, 0.45)" : "rgba(120, 150, 220, 0.22)";
	        }

          function setRecUi(state) {
            isRecording = state;
            toggleBtn.textContent = state ? "Stop" : "Start";
            toggleBtn.classList.toggle("recOn", state);
          }

          function setBusy(state) {
            isBusy = state;
            toggleBtn.disabled = state;
            newConvBtn.disabled = state || isRecording;
            speak.disabled = state || isRecording;
          }

          function fmtMs(ms) {
            if (ms === null || ms === undefined) return "—";
            const s = ms / 1000.0;
            if (s < 10) return `${s.toFixed(2)}s`;
            return `${s.toFixed(1)}s`;
          }

          function setLatencyFromMetrics(m) {
            const t = m.timings_ms || {};
            const asr = fmtMs(t.asr);
            const llm1 = fmtMs(t.llm_ttfb);
            const tts1 = t.tts_first_audio !== undefined ? fmtMs(t.tts_first_audio) : "—";
            const total = fmtMs(t.total);
            latencyPill.textContent = `latency: ASR ${asr} | LLM 1st ${llm1} | TTS 1st ${tts1} | total ${total}`;
          }

          function scrollToBottom() {
            // Ensure the latest bubble is visible.
            const last = chat.lastElementChild;
            if (last && last.scrollIntoView) last.scrollIntoView({ block: "end" });
            chat.scrollTop = chat.scrollHeight;
          }

        function appendBubble(role, text) {
          const div = document.createElement("div");
          div.style.maxWidth = "92%";
          div.style.alignSelf = role === "user" ? "flex-end" : "flex-start";
          div.style.padding = "10px 12px";
          div.style.border = "1px solid rgba(120,150,220,0.20)";
          div.style.borderRadius = "14px";
          div.style.background = role === "user" ? "rgba(124,135,255,0.12)" : "rgba(155,209,255,0.08)";
          div.style.whiteSpace = "pre-wrap";
          div.textContent = text || "";
          chat.appendChild(div);
          scrollToBottom();
          return div;
        }

        function appendAssistantWithMeta(initialText) {
          const bubble = appendBubble("assistant", initialText || "");
          const meta = document.createElement("div");
          meta.className = "muted";
          meta.style.marginTop = "-6px";
          meta.style.alignSelf = "flex-start";
          meta.style.fontSize = "11px";
          meta.textContent = "";
          chat.appendChild(meta);
          scrollToBottom();
          return { bubble, meta };
        }

        function b64ToArrayBuffer(b64) {
          const bin = atob(b64);
          const bytes = new Uint8Array(bin.length);
          for (let i = 0; i < bin.length; i++) bytes[i] = bin.charCodeAt(i);
          return bytes.buffer;
        }

        async function queueWav(b64) {
          if (!playCtx) playCtx = new (window.AudioContext || window.webkitAudioContext)();
          const buf = await playCtx.decodeAudioData(b64ToArrayBuffer(b64));
          const src = playCtx.createBufferSource();
          src.buffer = buf;
          src.connect(playCtx.destination);
          if (nextPlayTime < playCtx.currentTime) nextPlayTime = playCtx.currentTime;
          src.start(nextPlayTime);
          nextPlayTime += buf.duration;
        }

        function mergeFloat32(chunks) {
          let total = 0;
          for (const c of chunks) total += c.length;
          const out = new Float32Array(total);
          let offset = 0;
          for (const c of chunks) {
            out.set(c, offset);
            offset += c.length;
          }
          return out;
        }

        function resampleLinear(input, inRate, outRate) {
          if (inRate === outRate) return input;
          const ratio = outRate / inRate;
          const outLen = Math.round(input.length * ratio);
          const out = new Float32Array(outLen);
          for (let i = 0; i < outLen; i++) {
            const pos = i / ratio;
            const idx = Math.floor(pos);
            const frac = pos - idx;
            const s0 = input[idx] || 0;
            const s1 = input[idx + 1] || s0;
            out[i] = s0 + (s1 - s0) * frac;
          }
          return out;
        }

	        function encodeWavPCM16(samples, sampleRate) {
          const buffer = new ArrayBuffer(44 + samples.length * 2);
          const view = new DataView(buffer);
          function writeStr(off, s) { for (let i = 0; i < s.length; i++) view.setUint8(off + i, s.charCodeAt(i)); }
          writeStr(0, "RIFF");
          view.setUint32(4, 36 + samples.length * 2, true);
          writeStr(8, "WAVE");
          writeStr(12, "fmt ");
          view.setUint32(16, 16, true);
          view.setUint16(20, 1, true);
          view.setUint16(22, 1, true);
          view.setUint32(24, sampleRate, true);
          view.setUint32(28, sampleRate * 2, true);
          view.setUint16(32, 2, true);
          view.setUint16(34, 16, true);
          writeStr(36, "data");
          view.setUint32(40, samples.length * 2, true);
          let offset = 44;
          for (let i = 0; i < samples.length; i++, offset += 2) {
            let s = Math.max(-1, Math.min(1, samples[i]));
            view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7fff, true);
          }
	          return buffer;
	        }

	        function floatToPcm16(samples) {
	          const out = new Int16Array(samples.length);
	          for (let i = 0; i < samples.length; i++) {
	            let s = Math.max(-1, Math.min(1, samples[i]));
	            out[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
	          }
	          return out;
	        }

	        async function ensureWs() {
	          if (ws && ws.readyState === WebSocket.OPEN) return ws;
	          setStatus("connecting");
	          const proto = location.protocol === "https:" ? "wss" : "ws";
	          ws = new WebSocket(`${proto}://${location.host}/ws/bots/${botId}/talk`);
	          ws.binaryType = "arraybuffer";
            ws.onmessage = async (ev) => {
              let msg = null;
              try { msg = JSON.parse(ev.data); } catch { return; }
              const rid = msg.req_id || "";
              if (!rid) return;
              const h = wsHandlers.get(rid);
              if (h) h(msg);
            };
	          await new Promise((resolve, reject) => {
	            ws.onopen = resolve;
	            ws.onerror = reject;
	          });
	          setStatus("connected");
	          return ws;
	        }

          async function initConversation() {
            const socket = await ensureWs();
            const reqId = (crypto.randomUUID && crypto.randomUUID()) || String(Date.now());
            setBusy(true);
            setStatus("init");
            latencyPill.textContent = "latency: —";

            const assistant = appendAssistantWithMeta("");
            let assistantText = "";

            return await new Promise((resolve) => {
              wsHandlers.set(reqId, async (msg) => {
                if (msg.type === "status") {
                  setStatus(msg.stage);
                } else if (msg.type === "conversation") {
                  setConversation(msg.id);
                } else if (msg.type === "text_delta") {
                  assistantText += msg.delta;
                  assistant.bubble.textContent = assistantText;
                  scrollToBottom();
                } else if (msg.type === "audio_wav") {
                  await queueWav(msg.wav_base64);
                } else if (msg.type === "metrics") {
                  setLatencyFromMetrics(msg);
                  assistant.meta.textContent = latencyPill.textContent.replace("latency: ", "");
                } else if (msg.type === "error") {
                  assistant.bubble.textContent += `\\n[error] ${msg.error}`;
                  setStatus("error");
                  setBusy(false);
                  wsHandlers.delete(reqId);
                  resolve(false);
                } else if (msg.type === "done") {
                  setStatus("idle");
                  setBusy(false);
                  wsHandlers.delete(reqId);
                  resolve(true);
                }
              });
              socket.send(JSON.stringify({ type: "init", req_id: reqId, test_flag: true, speak: speak.checked }));
            });
          }

	        async function startRec() {
            if (isBusy) return;
            if (!conversationId) {
              const ok = await initConversation();
              // Important: don't start recording while the assistant is greeting / speaking.
              // User can press Start again after the conversation is initialized.
              return;
            }
	          chunks = [];
	          setStatus("requesting mic");
	          mediaStream = await navigator.mediaDevices.getUserMedia({
              audio: { echoCancellation: true, noiseSuppression: true, autoGainControl: true },
            });
	          audioCtx = new (window.AudioContext || window.webkitAudioContext)();
	          inputSampleRate = audioCtx.sampleRate || 48000;
          sourceNode = audioCtx.createMediaStreamSource(mediaStream);
          const zeroGain = audioCtx.createGain();
          zeroGain.gain.value = 0.0;

          recorderIsWorklet = false;
          if (audioCtx.audioWorklet && window.AudioWorkletNode) {
            try {
              await audioCtx.audioWorklet.addModule("/static/recorder-worklet.js");
              processor = new AudioWorkletNode(audioCtx, "recorder-worklet");
              processor.port.onmessage = (ev) => {
                const data = ev.data;
                const arr = (data instanceof Float32Array) ? data : new Float32Array(data);
                chunks.push(arr);
              };
              processor.port.postMessage({ cmd: "start" });
              sourceNode.connect(processor);
              processor.connect(zeroGain);
              recorderIsWorklet = true;
            } catch (e) {
              recorderIsWorklet = false;
            }
          }

	          if (!recorderIsWorklet) {
	            processor = audioCtx.createScriptProcessor(4096, 1, 1);
	            processor.onaudioprocess = (e) => {
	              const input = e.inputBuffer.getChannelData(0);
	              chunks.push(new Float32Array(input));
	            };
	            sourceNode.connect(processor);
	            processor.connect(zeroGain);
	          }

	          zeroGain.connect(audioCtx.destination);
            setRecUi(true);
	          setStatus("recording");
	        }

	        async function stopRec() {
            if (!isRecording || isBusy) return;
            setRecUi(false);
            setBusy(true);
	          setStatus("processing audio");
            try { if (recorderIsWorklet && processor && processor.port) processor.port.postMessage({ cmd: "stop" }); } catch {}
            if (recorderIsWorklet) { try { await new Promise(r => setTimeout(r, 60)); } catch {} }
	          try { processor.disconnect(); } catch {}
	          try { sourceNode.disconnect(); } catch {}
	          try { mediaStream.getTracks().forEach(t => t.stop()); } catch {}
	          try { await audioCtx.close(); } catch {}

	          const merged = mergeFloat32(chunks);
	          if (!merged.length) {
	            appendBubble("user", "(no audio)");
	            setStatus("idle");
              setBusy(false);
	            return;
	          }
	          const resampled = resampleLinear(merged, inputSampleRate, 16000);
	          const pcm16 = floatToPcm16(resampled);

	          const userBubble = appendBubble("user", "…");
	          const assistant = appendAssistantWithMeta("");
	          let assistantText = "";
	          const reqId = (crypto.randomUUID && crypto.randomUUID()) || String(Date.now());
            latencyPill.textContent = "latency: —";

	          const socket = await ensureWs();
            wsHandlers.set(reqId, async (msg) => {
              if (msg.type === "status") {
                setStatus(msg.stage);
              } else if (msg.type === "conversation") {
                setConversation(msg.id);
              } else if (msg.type === "asr") {
                userBubble.textContent = msg.text || "(no speech recognized)";
                scrollToBottom();
              } else if (msg.type === "text_delta") {
                assistantText += msg.delta;
                assistant.bubble.textContent = assistantText;
                scrollToBottom();
              } else if (msg.type === "metrics") {
                setLatencyFromMetrics(msg);
                assistant.meta.textContent = latencyPill.textContent.replace("latency: ", "");
              } else if (msg.type === "audio_wav") {
                await queueWav(msg.wav_base64);
              } else if (msg.type === "error") {
                assistant.bubble.textContent += `\\n[error] ${msg.error}`;
                setStatus("error");
                setBusy(false);
                wsHandlers.delete(reqId);
              } else if (msg.type === "done") {
                setStatus("idle");
                setBusy(false);
                wsHandlers.delete(reqId);
              }
            });

	          setStatus("sending");
	          socket.send(JSON.stringify({ type: "start", req_id: reqId, conversation_id: conversationId, test_flag: true, speak: speak.checked }));
	          socket.send(pcm16.buffer);
	          socket.send(JSON.stringify({ type: "stop", req_id: reqId }));
	        }

        toggleBtn.addEventListener("click", async () => {
          try {
            if (!isRecording) await startRec();
            else await stopRec();
          } catch (e) {
            setRecUi(false);
            setBusy(false);
            alert("Mic permission denied.");
          }
        });
        newConvBtn.addEventListener("click", async () => {
          setConversation("");
          chat.innerHTML = "";
          nextPlayTime = 0;
          await initConversation();
        });

        function updateStartMessagePlaceholder() {
          const mode = document.getElementById("start_message_mode").value;
          const ta = document.getElementById("start_message_text");
          const empty = !ta.value || !ta.value.trim();
          if (mode === "llm" || empty) {
            ta.placeholder = "will be generated by LLM";
          } else {
            ta.placeholder = "e.g. Hi! How can I help you today?";
          }
        }
        document.getElementById("start_message_mode").addEventListener("change", updateStartMessagePlaceholder);
        document.getElementById("start_message_text").addEventListener("input", updateStartMessagePlaceholder);
        updateStartMessagePlaceholder();

        async function loadSpeakers() {
          const model = document.getElementById("xtts_model").value;
          const sel = document.getElementById("speaker_id");
          const current = "{{ bot.speaker_id or '' }}";
          sel.innerHTML = '<option value="">(auto)</option>';
          try {
            const r = await fetch(`/api/tts/meta?model_name=${encodeURIComponent(model)}`);
            if (!r.ok) return;
            const data = await r.json();
            const speakers = data.speakers || [];
            for (const s of speakers) {
              const opt = document.createElement("option");
              opt.value = s;
              opt.textContent = s;
              if (current && s === current) opt.selected = true;
              sel.appendChild(opt);
            }
          } catch {}
        }
        document.getElementById("xtts_model").addEventListener("change", loadSpeakers);
        loadSpeakers();
      </script>

      <style>
        #chat { max-height: 62vh; overflow-y: auto; padding-right: 6px; }
        .recBtn.recOn {
          border-color: rgba(255, 94, 140, 0.55);
          background: linear-gradient(180deg, rgba(64, 16, 32, 0.9), rgba(28, 10, 20, 0.9));
          animation: recPulse 1.2s ease-in-out infinite;
        }
        @keyframes recPulse {
          0% { box-shadow: 0 0 0 rgba(255, 94, 140, 0.0); transform: translateY(0); }
          50% { box-shadow: 0 0 18px rgba(255, 94, 140, 0.18); transform: translateY(-1px); }
          100% { box-shadow: 0 0 0 rgba(255, 94, 140, 0.0); transform: translateY(0); }
        }
      </style>
    </div>
  </div>
{% endblock %}
